# TO BE RUN WIH LINUX ! (Because usage of the diff command)
# This is the main for the data preparation
import os.path
import subprocess
import numpy as np
import rundiff
import create_train_val_test_dataset

MAIN_FOLDER = '/mnt/docker/test_main_python_5CWE_req_only_token150_InferOK_obfuscated'
MAX_TOKEN_AUTHORIZED = 150
DIFF_LINES_AUTHORIZED = 99999999999999

def step1():
    print("STEP 1 : define the test cases separated folder")


def step2(test_case_separated_name_folder, removed_comments_and_blank_lines_name_folder):
    print("STEP 2 : remove comments & blank lines")
    try:
        os.mkdir(os.path.join(MAIN_FOLDER, removed_comments_and_blank_lines_name_folder))
    except:
        pass

    rundiff.main_remove_comments_and_blank_lines(
        os.path.join(MAIN_FOLDER, test_case_separated_name_folder),
        os.path.join(MAIN_FOLDER, removed_comments_and_blank_lines_name_folder))


def step3(removed_comments_and_blank_lines_name_folder, diff_lines_name_folder, diff_lines_authorized):
    print(f"STEP 3 : Select files with result of diff command less than {diff_lines_authorized}")
    try:
        os.mkdir(os.path.join(MAIN_FOLDER, diff_lines_name_folder))
        os.mkdir(os.path.join(MAIN_FOLDER, diff_lines_name_folder, "goods"))
        os.mkdir(os.path.join(MAIN_FOLDER, diff_lines_name_folder, "bads"))
    except:
        pass

    rundiff.main_run_diff_AND_copy_interesting_files(
        os.path.join(MAIN_FOLDER, removed_comments_and_blank_lines_name_folder),
        diff_lines_authorized,
        os.path.join(MAIN_FOLDER, diff_lines_name_folder, "goods"),
        os.path.join(MAIN_FOLDER, diff_lines_name_folder, "bads")
    )


def step4(diff_lines_name_folder, splitted_token_and_unique_txt_name_folder):
    print(f"STEP 4 : Split the tokens and create unique txt files (with the log file)")
    try:
        os.mkdir(os.path.join(MAIN_FOLDER, splitted_token_and_unique_txt_name_folder))
    except:
        pass

    create_train_val_test_dataset.split_tokens_folder(
        os.path.join(MAIN_FOLDER, diff_lines_name_folder, "bads"),
        os.path.join(MAIN_FOLDER, splitted_token_and_unique_txt_name_folder),
        "bads")

    create_train_val_test_dataset.split_tokens_folder(
        os.path.join(MAIN_FOLDER, diff_lines_name_folder, "goods"),
        os.path.join(MAIN_FOLDER, splitted_token_and_unique_txt_name_folder),
        "goods")

def step4bis(splitted_token_and_unique_txt_name_folder, filtered_max_tokens_name_folder, max_tokens_authorized):
    print(f"STEP 4 bis : Filter on the number of tokens")
    try:
        os.mkdir(os.path.join(MAIN_FOLDER, filtered_max_tokens_name_folder))
    except:
        pass

    # Count and log the number of tokens in the bad file
    cmd = "awk '{ print NF }' " + os.path.join(MAIN_FOLDER,splitted_token_and_unique_txt_name_folder,'bads.txt')  # Count nb of token for each line of the txt file
    results = subprocess.check_output(cmd, shell=True)
    all_counts_bads = [int(result) for result in (str(results)[2:-1]).split("\\n") if result != '']
    with open(os.path.join(MAIN_FOLDER,splitted_token_and_unique_txt_name_folder,'nb_tokens_bads.txt'), 'a') as destfile:
        for token_number in all_counts_bads:
            destfile.write(str(token_number)+"\n")

    # Count and log the number of tokens in the good file
    cmd = "awk '{ print NF }' " + os.path.join(MAIN_FOLDER, splitted_token_and_unique_txt_name_folder,'goods.txt')  # Count nb of token for each line of the txt file
    results = subprocess.check_output(cmd, shell=True)
    all_counts_goods = [int(result) for result in (str(results)[2:-1]).split("\\n") if result != '']
    with open(os.path.join(MAIN_FOLDER, splitted_token_and_unique_txt_name_folder, 'nb_tokens_goods.txt'),
              'a') as destfile:
        for token_number in all_counts_goods:
            destfile.write(str(token_number) + "\n")

    i_file_ok = [i for i,_ in enumerate(all_counts_goods) if all_counts_bads[i] <= max_tokens_authorized and all_counts_goods[i] <= max_tokens_authorized]
    new_bads_txt_list = [line for i, line in enumerate(open(os.path.join(MAIN_FOLDER,splitted_token_and_unique_txt_name_folder, 'bads.txt'), "r")) if
                         i in i_file_ok]
    new_goods_txt_list = [line for i, line in enumerate(open(os.path.join(MAIN_FOLDER,splitted_token_and_unique_txt_name_folder, 'goods.txt'), "r")) if
                          i in i_file_ok]
    new_log_bads_txt_list = [line for i, line in enumerate(open(os.path.join(MAIN_FOLDER,splitted_token_and_unique_txt_name_folder, 'log_bads.txt'), "r")) if
                             i in i_file_ok]
    new_log_goods_txt_list = [line for i, line in enumerate(open(os.path.join(MAIN_FOLDER,splitted_token_and_unique_txt_name_folder, 'log_goods.txt'), "r")) if
                              i in i_file_ok]


    np.savetxt(os.path.join(MAIN_FOLDER,filtered_max_tokens_name_folder,'bads.txt'), new_bads_txt_list, fmt="%s", newline="")
    np.savetxt(os.path.join(MAIN_FOLDER,filtered_max_tokens_name_folder,'goods.txt'), new_goods_txt_list, fmt="%s", newline="")
    np.savetxt(os.path.join(MAIN_FOLDER,filtered_max_tokens_name_folder,'log_bads.txt'), new_log_bads_txt_list, fmt="%s", newline="")
    np.savetxt(os.path.join(MAIN_FOLDER,filtered_max_tokens_name_folder,'log_goods.txt'), new_log_goods_txt_list, fmt="%s", newline="")

def step5(splitted_token_and_unique_txt_name_folder, final_dataset_name_folder):
    print(f"STEP 5 : Creation of the train/val/test dataset")
    try:
        os.mkdir(os.path.join(MAIN_FOLDER, final_dataset_name_folder))
    except:
        pass

    create_train_val_test_dataset.train_val_test(
        os.path.join(MAIN_FOLDER, splitted_token_and_unique_txt_name_folder),
        os.path.join(MAIN_FOLDER, final_dataset_name_folder))


def main():
    # 1) Use the notebook to extract the splitted CWE (cwe_folder_xxxx/{goodG2BX.txt,bad.txt,...}
    # Copy the splitted CWEs in the 'test_case_separated_name_folder'
    test_case_separated_name_folder = '1_Test_cases_separated'
    step1()

    # 2) remove comments & blank lines
    removed_comments_and_blank_lines_name_folder = '2_Removed_comments_and_blanks_lines'
    step2(test_case_separated_name_folder, removed_comments_and_blank_lines_name_folder)

    # 3) Select files with result of 'diff' command less than X
    diff_lines_authorized = DIFF_LINES_AUTHORIZED
    diff_lines_name_folder = f'3_diff_less_than_{diff_lines_authorized}'
    step3(removed_comments_and_blank_lines_name_folder, diff_lines_name_folder, diff_lines_authorized)

    # 4) Split the tokens,
    # gather the goods and bads files into on unique txt file
    # and create the associated log file
    splitted_token_and_unique_txt_name_folder = f'4_Splitted_tokens_and_unique_txt_file'
    step4(diff_lines_name_folder, splitted_token_and_unique_txt_name_folder)

    # 4bis) Filter on the max number of tokens
    max_tokens_authorized = MAX_TOKEN_AUTHORIZED
    filtered_max_tokens_name_folder = f'4bis_Filtered_max_tokens_{max_tokens_authorized}'
    step4bis(splitted_token_and_unique_txt_name_folder, filtered_max_tokens_name_folder,max_tokens_authorized)

    # 5) Create the train/val/test dataset
    filtered_max_tokens_name_folder = f'4bis_Filtered_max_tokens_{MAX_TOKEN_AUTHORIZED}'
    final_dataset_name_folder = '5_Final_dataset'
    step5(filtered_max_tokens_name_folder, final_dataset_name_folder)


if __name__ == "__main__":
    main()
