# Can be use to create a test set from a java file
# containing diverse methods
# RUN FROM LINUX

import os
import re
import subprocess

import numpy as np

import rundiff
import create_train_val_test_dataset

MAX_TOKEN_AUTHORIZED = 100

def split_file(filename, dest_folder_path):
    # Créer le dossier du test case
    try:
        os.mkdir(os.path.join(dest_folder_path, os.path.splitext(filename)[0]))
    except:
        pass

    textfile = open(filename, 'r')
    reg_method_words = 'public static'
    reg_types = r"(?:String|Object|int|boolean|float|short)[\[\]]+" # ?: = non capturing group
    reg_method_name = r"[a-zA-Z]*"
    reg_method_args = r"\([a-zA-Z\[\]\s,]*\)"
    words_to_detect = f'({reg_method_words}) ({reg_types}) ({reg_method_name})({reg_method_args})'

    dict_method_occur = {} # to add an incr value in for the files of the same method name

    start_count = False  # Use to know if we entered a good/bad method or another one (like main)
    count_brackets = 0
    all_lines = ""
    for line in textfile:
        all_lines += line
        if re.search(words_to_detect, line):
            start_count = True
            words_detected = re.search(words_to_detect, line)
            #print(words_detected.groups())
            return_type = words_detected.group(2)
            method_name = words_detected.group(3)

            if method_name in dict_method_occur:
                dict_method_occur[method_name] = dict_method_occur[method_name] + 1
            else:
                dict_method_occur[method_name] = 0

            f = open(os.path.join(dest_folder_path, method_name + "_" + str(dict_method_occur[method_name]) + ".txt"),
                     "w")
            all_lines = line
            count_brackets = 0
        if re.search("{", line):
            count_brackets = count_brackets + 1
        if re.search("}", line):
            count_brackets = count_brackets - 1
            if (count_brackets == 0 and start_count == True):
                start_count = False
                print("fin de la méthode")
                print(all_lines)
                f.write(all_lines)
                f.close()

    print(dict_method_occur)
    textfile.close()


if __name__ == "__main__":
    #file_for_testset_path = "C:/Users/benjamin.petit/Documents/JCrashPack/selected_java_files/ArrayUtils.java"
    file_for_testset_path= "/mnt/docker/new_testset/ArrayUtils.java"
    main_folder = os.path.splitext(file_for_testset_path)[0]
    filename = main_folder.split('/')[-1]
    print(filename)
    separated_methods_folder_path = os.path.join(main_folder,"1_separated_methods")
    removed_comments_and_blanks_lines_folder_path = os.path.join(main_folder, "2_removed_comments_and_blanks_lines")
    splitted_tokens_methods_folder_path = os.path.join(main_folder, "3_splitted_tokens")
    filtered_tokens_methods_folder_path = os.path.join(main_folder, "4_final_testset")

    try:
        os.mkdir(main_folder)
    except:
        pass
    try:
        os.mkdir(separated_methods_folder_path)
    except:
        pass
    split_file(file_for_testset_path, separated_methods_folder_path)

    try:
        os.mkdir(removed_comments_and_blanks_lines_folder_path)
    except:
        pass
    os.chdir(separated_methods_folder_path)
    for file in os.listdir():
        source_file = open(file, 'r').read()
        dest_file = open(os.path.join(removed_comments_and_blanks_lines_folder_path, file), 'w')
        dest_file.write(rundiff.remove_comments_from_file(source_file))
        dest_file.close()
        rundiff.remove_blank_lines(os.path.join(removed_comments_and_blanks_lines_folder_path, file))

    # Séparer tokens
    try:
        os.mkdir(splitted_tokens_methods_folder_path)
    except:
        pass
    logfile_path = f"{splitted_tokens_methods_folder_path}/log_{filename}.txt"
    os.chdir(removed_comments_and_blanks_lines_folder_path)
    files = os.listdir()

    for file in sorted(files):
        with open(f'{splitted_tokens_methods_folder_path}/{filename}.txt', 'a') as destfile:
            destfile.write(' '.join(create_train_val_test_dataset.split_tokens_file(file)) + "\n")
        with open(logfile_path, 'a') as logfile:
            logfile.write(f'{file}\n')

    try:
        os.mkdir(filtered_tokens_methods_folder_path)
    except:
        pass

    # Count and log the number of tokens in the bad file
    cmd = "awk '{ print NF }' " + os.path.join(f"{splitted_tokens_methods_folder_path}/{filename}.txt")
    results = subprocess.check_output(cmd, shell=True)
    all_counts = [int(result) for result in (str(results)[2:-1]).split("\\n") if result != '']
    with open(os.path.join(splitted_tokens_methods_folder_path,'nb_tokens.txt'), 'a') as destfile:
        for token_number in all_counts:
            destfile.write(str(token_number)+"\n")

    i_file_ok = [i for i,_ in enumerate(all_counts) if all_counts[i] <= MAX_TOKEN_AUTHORIZED]
    new_txt_list = [line for i, line in enumerate(open(f'{splitted_tokens_methods_folder_path}/{filename}.txt', "r")) if
                         i in i_file_ok]
    new_log_txt_list = [line for i, line in enumerate(open(f"{splitted_tokens_methods_folder_path}/log_{filename}.txt", "r")) if
                             i in i_file_ok]


    np.savetxt(os.path.join(filtered_tokens_methods_folder_path,'testset.txt'), new_txt_list, fmt="%s", newline="")
    np.savetxt(os.path.join(filtered_tokens_methods_folder_path,'log_testset.txt'), new_log_txt_list, fmt="%s", newline="")
