import re
import os
import pandas
import numpy as np
import shutil
from sklearn.model_selection import KFold

import rundiff


# pandas.options.display.max_colwidth = 10000


def split_tokens_file(file):
    with open(file, 'r') as f:
        # Separate the words and the special operators (!=)
        # splited_file = re.split('(!=)|(==)|(".*?")|([^a-zA-Z0-9_])', f.read())
        splited_file = re.split('(!=)|(==)|(")|(&&)|([^a-zA-Z0-9_\\\])', f.read())
        # Remove empty elements, spaces and \n
        # print(*[x for x in splited_file if (x and x != ' ' and x != '\n')])
        return [x for x in splited_file if (x and x != ' ' and x != '\n')]


def split_tokens_folder(source_folder, dest_folder, name_dest_file):
    """
    Split all the files from a folder
    usage : python3.9 split_token.py > CWE89_less_than_6diff_bads.txt
    :param folder: Path to the folder with containing all the .txt files
    :return: print the file
    """

    logfile_path = f'{dest_folder}/log_{name_dest_file}.txt'
    os.chdir(source_folder)
    files = os.listdir()

    for file in sorted(files):
        with open(f'{dest_folder}/{name_dest_file}.txt', 'a') as destfile:
            destfile.write(' '.join(split_tokens_file(file)) + "\n")
        with open(logfile_path, 'a') as logfile:
            logfile.write(f'{file}\n')


def create_test_set_from_java_project(src_folder, dest_folder):
    # Find the java files and split them by methods or nb of tokens or char or ...
    first_folder = "1_java_files"
    try:
        os.mkdir(os.path.join(dest_folder, first_folder))
    except:
        pass
    os.chdir(src_folder)
    for root, dirs, files in os.walk("."):
        for file in files:
            if file.endswith(".java"):
                # Probably futur work : Split by methods or nb of tokens

                with open(file, 'r') as file_opened:
                    start_count = False
                    all_lines = ""
                    count_brackets = 0
                    for line in file_opened:
                        all_lines += line
                        if re.search('public static void (?!main)', line):
                            start_count = True
                            method_name = [token for token in line.split(" ") if token != ''][3][:-2]
                            f = open(os.path.join(dest_folder, first_folder, file + "___" + method_name + ".txt"), 'w')
                            all_lines = line
                            count_brackets = 0
                        if re.search("{", line):
                            count_brackets = count_brackets + 1
                        if re.search("}", line):
                            count_brackets = count_brackets - 1
                            if (count_brackets == 0 and start_count == True):
                                start_count = False
                                # print("fin de la méthode")
                                f.write(all_lines)
                                f.close()
                # shutil.copyfile(os.path.join(root, file), os.path.join(dest_folder, first_folder,file))

    # Remove comments & blank lines
    sec_folder = "2_java_files_without_comments_and_blank_lines"
    try:
        os.mkdir(os.path.join(dest_folder, sec_folder))
    except:
        pass

    os.chdir(os.path.join(dest_folder, first_folder))
    for file in os.listdir():
        source_file = open(file, 'r').read()
        dest_file = open(os.path.join(dest_folder, sec_folder, file), 'w')
        dest_file.write(rundiff.remove_comments_from_file(source_file))
        dest_file.close()
        rundiff.remove_blank_lines(os.path.join(dest_folder, sec_folder, file))

    # Séparer tokens
    third_folder = "3_java_files_tokenized"
    try:
        os.mkdir(os.path.join(dest_folder, third_folder))
    except:
        pass
    split_tokens_folder(
        os.path.join(dest_folder, sec_folder),
        os.path.join(dest_folder, third_folder),
        "java_pred")


def train_val_test(folder_source, folder_to_save):
    """
    :param url_bad_file: .txt with all "tokens splitted" bad files gathered
    :param url_good_file: .txt with all "tokens splitted" good files gathered
    :param folder_to_save: folder where save the 6 news files
    :return: 6 files corresponding to goods and bads train(70%)/val(20%)/test(10%) datasets + log files
    """
    file_bads = open(os.path.join(folder_source, 'bads.txt'), "r")
    file_goods = open(os.path.join(folder_source, 'goods.txt'), "r")
    log_file_bads = open(os.path.join(folder_source, 'log_bads.txt'), 'r')
    log_file_goods = open(os.path.join(folder_source, 'log_goods.txt'), 'r')
    list_of_bads = []
    list_of_goods = []
    list_log_bads = []
    list_log_goods = []

    for line in file_bads:
        list_of_bads.append(line)

    for line in file_goods:
        list_of_goods.append(line)

    for line in log_file_bads:
        list_log_bads.append(line)

    for line in log_file_goods:
        list_log_goods.append(line)

    file_bads.close()
    file_goods.close()
    log_file_bads.close()
    log_file_goods.close()

    data = pandas.DataFrame()
    data['goods'] = list_of_goods
    data['bads'] = list_of_bads
    data['log_goods'] = list_log_goods
    data['log_bads'] = list_log_bads

    data = data[data['goods'] != '\n']
    data = data[data['bads'] != '\n']

    # Remove the duplicates in the goods methods
    data = data.drop_duplicates(subset=['goods'], keep="first")

    print(data['bads'].describe())
    print(data['goods'].describe())
    print(data.columns)


    #print(data[data.duplicated(['goods'], keep=False)]['log_goods'])
    #for log in data[data.duplicated(['goods'], keep=False)].sort_values("goods")['log_goods']:
    #    print(log)

    # print(data['goods'].sort_values())
    # print(data.iloc[400])

    """
    train, validate, test = np.split(data.sample(frac=1, random_state=42),
                                     [int(.7 * len(data)), int(.9 * len(data))])

    np.savetxt(f'{folder_to_save}/train_goods.txt', train['goods'].values, fmt="%s", newline="")
    np.savetxt(f'{folder_to_save}/train_bads.txt', train['bads'].values, fmt="%s", newline="")
    np.savetxt(f'{folder_to_save}/train_logs.txt', train['log_bads'].apply(lambda x : ('_'.join(x.split('_')[0:-1]) +".java")).values, fmt="%s", newline="\n")

    np.savetxt(f'{folder_to_save}/val_goods.txt', validate['goods'].values, fmt="%s", newline="")
    np.savetxt(f'{folder_to_save}/val_bads.txt', validate['bads'].values, fmt="%s", newline="")
    np.savetxt(f'{folder_to_save}/val_logs.txt', validate['log_bads'].apply(lambda x: ('_'.join(x.split('_')[0:-1]) +".java")).values, fmt="%s", newline="\n")

    np.savetxt(f'{folder_to_save}/test_goods.txt', test['goods'].values, fmt="%s", newline="")
    np.savetxt(f'{folder_to_save}/test_bads.txt', test['bads'].values, fmt="%s", newline="")
    np.savetxt(f'{folder_to_save}/test_logs.txt', test['log_bads'].apply(lambda x : ('_'.join(x.split('_')[0:-1]) +".java")).values, fmt="%s", newline="\n")
    """

    # https://stackoverflow.com/questions/45115964/separate-pandas-dataframe-using-sklearns-kfold
    kfold = KFold(n_splits=5, shuffle=True)
    for i, fold in enumerate(kfold.split(data)):
        try:
            os.mkdir(os.path.join(folder_to_save, f"fold{i}"))
        except:
            pass
        train = data.iloc[fold[0]]
        test = data.iloc[fold[1]]

        np.savetxt(f'{folder_to_save}/fold{i}/train_goods.txt', train['goods'].values, fmt="%s", newline="")
        np.savetxt(f'{folder_to_save}/fold{i}/train_bads.txt', train['bads'].values, fmt="%s", newline="")
        np.savetxt(f'{folder_to_save}/fold{i}/train_logs.txt',
                   train['log_bads'].apply(lambda x: ('_'.join(x.split('_')[0:-1]) + ".java")).values, fmt="%s",
                   newline="\n")

        np.savetxt(f'{folder_to_save}/fold{i}/test_goods.txt', test['goods'].values, fmt="%s", newline="")
        np.savetxt(f'{folder_to_save}/fold{i}/test_bads.txt', test['bads'].values, fmt="%s", newline="")
        np.savetxt(f'{folder_to_save}/fold{i}/test_logs.txt',
                   test['log_bads'].apply(lambda x: ('_'.join(x.split('_')[0:-1]) + ".java")).values, fmt="%s",
                   newline="\n")

def main():
    print("create_train_val_test_dataset : MAIN LAUNCHED")
    """
    bad_folder = "/mnt/docker/Test_cases_separated_NCB_req_meth_5CWE_without_comment_bad_less_than_6"
    good_folder = "/mnt/docker/Test_cases_separated_NCB_req_meth_5CWE_without_comment_good_less_than_6"
    dest_folder = "/mnt/docker/Test_cases_separated_NCB_req_meth_5CWE_without_comment_less_than_6_final_dataset"

    #Ne fonctionnera plus non plus : Il faut faire bad puis good
    split_tokens_folder(bad_folder,good_folder,dest_folder)
    """

    """
    create_test_set_from_java_project(
        "C:/Users/benjamin.petit/Documents/my_java_project/src",
        "C:/Users/benjamin.petit/Documents/my_java_project/to_predict")
    """
    print(split_tokens_file(
        "C:\\Users\\benjamin.petit\\Documents\\Juliet_Test_Suite_5CWE_req_only_OBFUSCATED\\CWE129_Improper_Validation_of_Array_Index__connect_tcp_array_read_check_max_01.java"))


if __name__ == "__main__":
    main()
